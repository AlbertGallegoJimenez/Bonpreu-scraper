# web-scraping-pr1

## DescripciÃ³n del repositorio

Este repositorio contiene el cÃ³digo para realizar _web scraping_ sobre la empresa de alimentaciÃ³n BonpreuEsclat.

<div align="center">
      <img src="image.png" width="50%">
</div>

El producto final de este proyecto se almacena en la carpeta [data](/data) en formato CSV, que incluye informaciÃ³n detallada sobre los productos en venta.
<br>Para obtener mÃ¡s informaciÃ³n sobre el enfoque metodolÃ³gico utilizado, la estructura del conjunto de datos resultante y otros detalles relevantes, consulta la documentaciÃ³n disponible en la carpeta [docs](/docs).

Este proyecto se presenta como la PrÃ¡ctica 1 de la asignatura M2.851 - TipologÃ­a y Ciclo de Vida de los Datos del MÃ¡ster de Data Science de la UOC.

## Estructura de la carpeta

- ğŸ“‚ [**data**](/data): Los datos obtenidos de la prÃ¡ctica.
- ğŸ“‚ [**docs**](/docs): Archivos de documentaciÃ³n.
- ğŸ“‚ [**src**](/src): CÃ³digo fuente.
- ğŸ“‚ [**tests**](/tests): Tests automatizados.
- ğŸ“„ **environment.yml**: Archivo YML para instalar un environment de Anaconda igual que el usado para el desarrollo del proyecto.
- ğŸ“„ **LICENSE**: Archivo de licencia.
- ğŸ“„ **requirements.txt**: Un archivo de requirements donde se detallan las librerÃ­as utilizadas con sus respectivas versiones.
- ğŸ“„ **README.md**: Este mismo archivo README.

## InstalaciÃ³n

0. Si usas [Anaconda](https://www.anaconda.com/) puedes crear un **environment** directamente desde el archivo **environment.yml** de la siguiente forma:
      ``` bash
      conda env create --file environment.yml
      conda activate web_scraping_env
      ```
   Este environment ya tiene especificada tanto la versiÃ³n de Python utilizada, como las librerÃ­as de las que depende. De esta forma, puedes saltarte el siguiente paso. 
   <br>

1. Si no usas Anaconda y/o no tienes instaladas las librerÃ­as usadas en el programa:
      ``` bash
      pip install -r requirements.txt
      ```

## CÃ³mo ejecutar el proyecto?

El proyecto se ejecuta desde el script principal (``main.py``) desde consola. A continuaciÃ³n, se detallan las opciones de ejecuciÃ³n disponibles.

### 0.1. Listar las categorÃ­as disponibles

Para ver todas las categorÃ­as que se pueden seleccionar, utiliza el siguiente comando:

```bash
python main.py --list-categories
```
Este comando solo muestra las categorÃ­as disponibles.

### 0.2. Listar las categorÃ­as disponibles

```bash
python main.py --category <NOMBRE_CATEGORÃA> --list-subcategories
```
Este comando solo muestra las subcategorÃ­as disponibles de una categorÃ­a dada.

### 1. Ejecutar el scraper en una categorÃ­a completa

Para ejecutar el scraper en una categorÃ­a completa (es decir, en todas sus subcategorÃ­as), usa:

```bash
python main.py --category <NOMBRE_CATEGORÃA>
```

Ejemplo:

```bash
python main.py --category Frescos
```

### 2. Ejecutar el scraper en subcategorÃ­as especÃ­ficas

Para ejecutar el scraper en subcategorÃ­as especÃ­ficas, usa:

```bash
python main.py --category <NOMBRE_CATEGORÃA> --subcategories <SUBCATEGORÃA_1> <SUBCATEGORÃA_2>
```

Ejemplo:

```bash
python main.py --category Frescos --subcategories "Fruites i verdura" "Xarcuteria"
```

### 3. Ejecutar el scraper en todas las categorÃ­as

Para ejecutar el scraper en todas las categorÃ­as disponible, usa:

```bash
python main.py --category all
```

Este comando ejecutarÃ¡ el scraper en todas las categorÃ­as y sus subcategorÃ­as.

> [!CAUTION]
>Si seleccionas la opciÃ³n de "todas las categorÃ­as" (`--category all`), utiliza el programa de forma responsable. Aunque el cÃ³digo implementa medidas para evitar la sobrecarga del servidor (como tiempos de espera), es importante moderar el uso para no saturar los recursos del sitio web de Bonpreu.


## Colaboradores
Este repositorio ha sido desarrollado por:
- ğŸ‘¨â€ğŸ’» David RoldÃ¡n Puig
- ğŸ‘¨â€ğŸ’» Albert Gallego JimÃ©nez
